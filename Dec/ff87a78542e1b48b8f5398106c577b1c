EVERY THREE years, as they await PISA results, education ministers get a taste of the tension they inflict on children. The Programme for International Student Assessment is a test of the reading, maths and science abilities of youngsters in 79 countries, and provides an impartial check on the progress of participating education systems. The data it produces are pored over by wonks as they hunt for the secrets of success, as well as by politicians seeking vindication for reforms they have implemented. The seventh and latest batch of results were published on December 3rd, nearly two decades after the first in 2000. Some 600,000 pupils—aged 15-16 and in formal education for at least six years—sat the tests. Participating countries include the (mostly rich) members of the OECD, which runs PISA, as well as 42 volunteers, ranging from Albania to Vietnam. The OECD adjusts each sample to be representative of the youth population of the country in question, producing results on a standardised scale.The hope at the turn of the millennium was that the wealth of new information would help identify what makes a school system tick, prompting others to follow their lead, and thus causing results to rise across the board. This is not quite how things have worked out. Despite the fact that spending per pupil in the OECD has risen by 15%, average performance in reading, maths and science remains essentially the same as when the tests started. Pick a country at random and it is just as likely to have regressed as improved.  As ever, this year’s results include plenty of bright spots (see chart). Singapore’s excellent results have improved once more. Even so, it is no longer the highest achiever. That is China—or to be more precise, Beijing, Shanghai, Jiangsu and Guangdong (the OECD declines to include results from farther afield because it cannot guarantee their veracity). In these parts of China the average pupil’s maths score is 591, compared with an OECD average of 489, suggesting local teenagers are roughly three years ahead of the OECD average. Mid-ranking countries, including Jordan, Poland and Turkey, have also improved. But for every Singapore, there is a Finland, once seen as an example to others—or another country that has seen its results fall. Part of the reason for the lack of overall progress is that schools have less influence over results than is commonly assumed, and culture and society more, meaning that even well-informed policymakers can only make so much difference. As John Jerrim of University College London notes, “You are always going to have East Asian countries coming top.” If a silver bullet for improving education existed it would have been discovered by now. Yet that does not mean improvement is impossible, or that there is nothing to learn from PISA. Plenty of countries have seen their results rise or fall without dramatic cultural change. And, as the data suggest, part of the reason for the lack of overall improvement, despite increased spending, is that above a certain level (around $60,000 per pupil, cumulatively between the ages of six and 15) there is not much of a relationship between expenditure and test scores. A big problem is that many education ministers still pay too little attention to the evidence. Others are hemmed in by the fact they must listen to the views of teachers and parents, who do not always know best. Andreas Schleicher, head of education at the OECD, bemoans the fact that lots of countries have, for instance, prioritised shrinking classes over hiring and training excellent teachers, despite evidence suggesting this is a bad idea. As he points out, one place that has given the quality of the teacher priority over the size of the class is Shanghai. Another is Singapore. And they are reaping the benefits.